{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root= 'data/Planetoid', name='Cora', transform = NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs 1\n",
      "Number of features 1433\n",
      "Number of classes 7\n"
     ]
    }
   ],
   "source": [
    "# Get some basic info about the dataset\n",
    "print(f'Number of graphs {len(dataset)}')\n",
    "print(f'Number of features {dataset.num_features}')\n",
    "print(f'Number of classes {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is only one graph in the dataset, use it as new data object\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Number of training nodes: 140\n",
      "Is Undirected: True\n"
     ]
    }
   ],
   "source": [
    "# Gather some statistics about the graph.\n",
    "print(data)\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Is Undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 4,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 633,    0],\n",
       "        [1862,    0],\n",
       "        [2582,    0],\n",
       "        ...,\n",
       "        [ 598, 2707],\n",
       "        [1473, 2707],\n",
       "        [2706, 2707]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ...,  True,  True,  True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Multi-layer Perception Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (lin1): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  (lin2): Linear(in_features=16, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(dataset.num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,p=0.3, training = self.training)\n",
    "        x= self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "model = MLP(hidden_channels = 16)\n",
    "print(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, loss: 1.9617\n",
      "Epoch: 002, loss: 1.9547\n",
      "Epoch: 003, loss: 1.9483\n",
      "Epoch: 004, loss: 1.9390\n",
      "Epoch: 005, loss: 1.9300\n",
      "Epoch: 006, loss: 1.9222\n",
      "Epoch: 007, loss: 1.9078\n",
      "Epoch: 008, loss: 1.9002\n",
      "Epoch: 009, loss: 1.8887\n",
      "Epoch: 010, loss: 1.8740\n",
      "Epoch: 011, loss: 1.8656\n",
      "Epoch: 012, loss: 1.8381\n",
      "Epoch: 013, loss: 1.8267\n",
      "Epoch: 014, loss: 1.8146\n",
      "Epoch: 015, loss: 1.7998\n",
      "Epoch: 016, loss: 1.7767\n",
      "Epoch: 017, loss: 1.7554\n",
      "Epoch: 018, loss: 1.7438\n",
      "Epoch: 019, loss: 1.7151\n",
      "Epoch: 020, loss: 1.6958\n",
      "Epoch: 021, loss: 1.6806\n",
      "Epoch: 022, loss: 1.6729\n",
      "Epoch: 023, loss: 1.6272\n",
      "Epoch: 024, loss: 1.5937\n",
      "Epoch: 025, loss: 1.5751\n",
      "Epoch: 026, loss: 1.5353\n",
      "Epoch: 027, loss: 1.5123\n",
      "Epoch: 028, loss: 1.4931\n",
      "Epoch: 029, loss: 1.4727\n",
      "Epoch: 030, loss: 1.4459\n",
      "Epoch: 031, loss: 1.4165\n",
      "Epoch: 032, loss: 1.3883\n",
      "Epoch: 033, loss: 1.3637\n",
      "Epoch: 034, loss: 1.3108\n",
      "Epoch: 035, loss: 1.3153\n",
      "Epoch: 036, loss: 1.2671\n",
      "Epoch: 037, loss: 1.2581\n",
      "Epoch: 038, loss: 1.2129\n",
      "Epoch: 039, loss: 1.1697\n",
      "Epoch: 040, loss: 1.1438\n",
      "Epoch: 041, loss: 1.1058\n",
      "Epoch: 042, loss: 1.0953\n",
      "Epoch: 043, loss: 1.0886\n",
      "Epoch: 044, loss: 1.0346\n",
      "Epoch: 045, loss: 0.9965\n",
      "Epoch: 046, loss: 0.9769\n",
      "Epoch: 047, loss: 0.9647\n",
      "Epoch: 048, loss: 0.8994\n",
      "Epoch: 049, loss: 0.8883\n",
      "Epoch: 050, loss: 0.8927\n",
      "Epoch: 051, loss: 0.8538\n",
      "Epoch: 052, loss: 0.8662\n",
      "Epoch: 053, loss: 0.8113\n",
      "Epoch: 054, loss: 0.8355\n",
      "Epoch: 055, loss: 0.7454\n",
      "Epoch: 056, loss: 0.7209\n",
      "Epoch: 057, loss: 0.7612\n",
      "Epoch: 058, loss: 0.7187\n",
      "Epoch: 059, loss: 0.6906\n",
      "Epoch: 060, loss: 0.7146\n",
      "Epoch: 061, loss: 0.6648\n",
      "Epoch: 062, loss: 0.6361\n",
      "Epoch: 063, loss: 0.6263\n",
      "Epoch: 064, loss: 0.5933\n",
      "Epoch: 065, loss: 0.5684\n",
      "Epoch: 066, loss: 0.5909\n",
      "Epoch: 067, loss: 0.5749\n",
      "Epoch: 068, loss: 0.5485\n",
      "Epoch: 069, loss: 0.5607\n",
      "Epoch: 070, loss: 0.5255\n",
      "Epoch: 071, loss: 0.5861\n",
      "Epoch: 072, loss: 0.5914\n",
      "Epoch: 073, loss: 0.5688\n",
      "Epoch: 074, loss: 0.4952\n",
      "Epoch: 075, loss: 0.5098\n",
      "Epoch: 076, loss: 0.4927\n",
      "Epoch: 077, loss: 0.4787\n",
      "Epoch: 078, loss: 0.4726\n",
      "Epoch: 079, loss: 0.4474\n",
      "Epoch: 080, loss: 0.4324\n",
      "Epoch: 081, loss: 0.4677\n",
      "Epoch: 082, loss: 0.4567\n",
      "Epoch: 083, loss: 0.4505\n",
      "Epoch: 084, loss: 0.4622\n",
      "Epoch: 085, loss: 0.4127\n",
      "Epoch: 086, loss: 0.3945\n",
      "Epoch: 087, loss: 0.4622\n",
      "Epoch: 088, loss: 0.4043\n",
      "Epoch: 089, loss: 0.3802\n",
      "Epoch: 090, loss: 0.4250\n",
      "Epoch: 091, loss: 0.4160\n",
      "Epoch: 092, loss: 0.3635\n",
      "Epoch: 093, loss: 0.3917\n",
      "Epoch: 094, loss: 0.3792\n",
      "Epoch: 095, loss: 0.3649\n",
      "Epoch: 096, loss: 0.3640\n",
      "Epoch: 097, loss: 0.4184\n",
      "Epoch: 098, loss: 0.3647\n",
      "Epoch: 099, loss: 0.3778\n",
      "Epoch: 100, loss: 0.4152\n",
      "Epoch: 101, loss: 0.3976\n",
      "Epoch: 102, loss: 0.3373\n",
      "Epoch: 103, loss: 0.3411\n",
      "Epoch: 104, loss: 0.4003\n",
      "Epoch: 105, loss: 0.3764\n",
      "Epoch: 106, loss: 0.3798\n",
      "Epoch: 107, loss: 0.3149\n",
      "Epoch: 108, loss: 0.3571\n",
      "Epoch: 109, loss: 0.3376\n",
      "Epoch: 110, loss: 0.3500\n",
      "Epoch: 111, loss: 0.3453\n",
      "Epoch: 112, loss: 0.3262\n",
      "Epoch: 113, loss: 0.3179\n",
      "Epoch: 114, loss: 0.3222\n",
      "Epoch: 115, loss: 0.2910\n",
      "Epoch: 116, loss: 0.3530\n",
      "Epoch: 117, loss: 0.2881\n",
      "Epoch: 118, loss: 0.3372\n",
      "Epoch: 119, loss: 0.2777\n",
      "Epoch: 120, loss: 0.3255\n",
      "Epoch: 121, loss: 0.3397\n",
      "Epoch: 122, loss: 0.3056\n",
      "Epoch: 123, loss: 0.3679\n",
      "Epoch: 124, loss: 0.2970\n",
      "Epoch: 125, loss: 0.2947\n",
      "Epoch: 126, loss: 0.3028\n",
      "Epoch: 127, loss: 0.3006\n",
      "Epoch: 128, loss: 0.2722\n",
      "Epoch: 129, loss: 0.3062\n",
      "Epoch: 130, loss: 0.3595\n",
      "Epoch: 131, loss: 0.2802\n",
      "Epoch: 132, loss: 0.3196\n",
      "Epoch: 133, loss: 0.2808\n",
      "Epoch: 134, loss: 0.2857\n",
      "Epoch: 135, loss: 0.2820\n",
      "Epoch: 136, loss: 0.3028\n",
      "Epoch: 137, loss: 0.2948\n",
      "Epoch: 138, loss: 0.2546\n",
      "Epoch: 139, loss: 0.3009\n",
      "Epoch: 140, loss: 0.2813\n",
      "Epoch: 141, loss: 0.2883\n",
      "Epoch: 142, loss: 0.2751\n",
      "Epoch: 143, loss: 0.2717\n",
      "Epoch: 144, loss: 0.3083\n",
      "Epoch: 145, loss: 0.2738\n",
      "Epoch: 146, loss: 0.2693\n",
      "Epoch: 147, loss: 0.2793\n",
      "Epoch: 148, loss: 0.2342\n",
      "Epoch: 149, loss: 0.2450\n",
      "Epoch: 150, loss: 0.2864\n",
      "Epoch: 151, loss: 0.2766\n",
      "Epoch: 152, loss: 0.2827\n",
      "Epoch: 153, loss: 0.3037\n",
      "Epoch: 154, loss: 0.3018\n",
      "Epoch: 155, loss: 0.3149\n",
      "Epoch: 156, loss: 0.2616\n",
      "Epoch: 157, loss: 0.2702\n",
      "Epoch: 158, loss: 0.2279\n",
      "Epoch: 159, loss: 0.2527\n",
      "Epoch: 160, loss: 0.2639\n",
      "Epoch: 161, loss: 0.2701\n",
      "Epoch: 162, loss: 0.2795\n",
      "Epoch: 163, loss: 0.2697\n",
      "Epoch: 164, loss: 0.2666\n",
      "Epoch: 165, loss: 0.2667\n",
      "Epoch: 166, loss: 0.2162\n",
      "Epoch: 167, loss: 0.2791\n",
      "Epoch: 168, loss: 0.2883\n",
      "Epoch: 169, loss: 0.2563\n",
      "Epoch: 170, loss: 0.2559\n",
      "Epoch: 171, loss: 0.2832\n",
      "Epoch: 172, loss: 0.2543\n",
      "Epoch: 173, loss: 0.2349\n",
      "Epoch: 174, loss: 0.2544\n",
      "Epoch: 175, loss: 0.2733\n",
      "Epoch: 176, loss: 0.2244\n",
      "Epoch: 177, loss: 0.2311\n",
      "Epoch: 178, loss: 0.2238\n",
      "Epoch: 179, loss: 0.2177\n",
      "Epoch: 180, loss: 0.2678\n",
      "Epoch: 181, loss: 0.2479\n",
      "Epoch: 182, loss: 0.2728\n",
      "Epoch: 183, loss: 0.2567\n",
      "Epoch: 184, loss: 0.2034\n",
      "Epoch: 185, loss: 0.3026\n",
      "Epoch: 186, loss: 0.2940\n",
      "Epoch: 187, loss: 0.2707\n",
      "Epoch: 188, loss: 0.2356\n",
      "Epoch: 189, loss: 0.2447\n",
      "Epoch: 190, loss: 0.2348\n",
      "Epoch: 191, loss: 0.2795\n",
      "Epoch: 192, loss: 0.2348\n",
      "Epoch: 193, loss: 0.2148\n",
      "Epoch: 194, loss: 0.2466\n",
      "Epoch: 195, loss: 0.2098\n",
      "Epoch: 196, loss: 0.2479\n",
      "Epoch: 197, loss: 0.2838\n",
      "Epoch: 198, loss: 0.2246\n",
      "Epoch: 199, loss: 0.2597\n",
      "Epoch: 200, loss: 0.2317\n",
      "Epoch: 201, loss: 0.2433\n",
      "Epoch: 202, loss: 0.2488\n",
      "Epoch: 203, loss: 0.2113\n",
      "Epoch: 204, loss: 0.2304\n",
      "Epoch: 205, loss: 0.1811\n",
      "Epoch: 206, loss: 0.2396\n",
      "Epoch: 207, loss: 0.2389\n",
      "Epoch: 208, loss: 0.2672\n",
      "Epoch: 209, loss: 0.2363\n",
      "Epoch: 210, loss: 0.2236\n",
      "Epoch: 211, loss: 0.2121\n",
      "Epoch: 212, loss: 0.2348\n",
      "Epoch: 213, loss: 0.1792\n",
      "Epoch: 214, loss: 0.2335\n",
      "Epoch: 215, loss: 0.2165\n",
      "Epoch: 216, loss: 0.1943\n",
      "Epoch: 217, loss: 0.1958\n",
      "Epoch: 218, loss: 0.2262\n",
      "Epoch: 219, loss: 0.2884\n",
      "Epoch: 220, loss: 0.2517\n",
      "Epoch: 221, loss: 0.2596\n",
      "Epoch: 222, loss: 0.2605\n",
      "Epoch: 223, loss: 0.2146\n",
      "Epoch: 224, loss: 0.1839\n",
      "Epoch: 225, loss: 0.2188\n",
      "Epoch: 226, loss: 0.2049\n",
      "Epoch: 227, loss: 0.2016\n",
      "Epoch: 228, loss: 0.2249\n",
      "Epoch: 229, loss: 0.2202\n",
      "Epoch: 230, loss: 0.2052\n",
      "Epoch: 231, loss: 0.2505\n",
      "Epoch: 232, loss: 0.2205\n",
      "Epoch: 233, loss: 0.2437\n",
      "Epoch: 234, loss: 0.2349\n",
      "Epoch: 235, loss: 0.1799\n",
      "Epoch: 236, loss: 0.1936\n",
      "Epoch: 237, loss: 0.2294\n",
      "Epoch: 238, loss: 0.1654\n",
      "Epoch: 239, loss: 0.1886\n",
      "Epoch: 240, loss: 0.2314\n",
      "Epoch: 241, loss: 0.2074\n",
      "Epoch: 242, loss: 0.2149\n",
      "Epoch: 243, loss: 0.2284\n",
      "Epoch: 244, loss: 0.2299\n",
      "Epoch: 245, loss: 0.1725\n",
      "Epoch: 246, loss: 0.2143\n",
      "Epoch: 247, loss: 0.1863\n",
      "Epoch: 248, loss: 0.2160\n",
      "Epoch: 249, loss: 0.2332\n",
      "Epoch: 250, loss: 0.2383\n",
      "Epoch: 251, loss: 0.2248\n",
      "Epoch: 252, loss: 0.1773\n",
      "Epoch: 253, loss: 0.2238\n",
      "Epoch: 254, loss: 0.2095\n",
      "Epoch: 255, loss: 0.1929\n",
      "Epoch: 256, loss: 0.2145\n",
      "Epoch: 257, loss: 0.1850\n",
      "Epoch: 258, loss: 0.2054\n",
      "Epoch: 259, loss: 0.2035\n",
      "Epoch: 260, loss: 0.1946\n",
      "Epoch: 261, loss: 0.2264\n",
      "Epoch: 262, loss: 0.2138\n",
      "Epoch: 263, loss: 0.1908\n",
      "Epoch: 264, loss: 0.2185\n",
      "Epoch: 265, loss: 0.2046\n",
      "Epoch: 266, loss: 0.1872\n",
      "Epoch: 267, loss: 0.2026\n",
      "Epoch: 268, loss: 0.2073\n",
      "Epoch: 269, loss: 0.1871\n",
      "Epoch: 270, loss: 0.2193\n",
      "Epoch: 271, loss: 0.1634\n",
      "Epoch: 272, loss: 0.2289\n",
      "Epoch: 273, loss: 0.2286\n",
      "Epoch: 274, loss: 0.1971\n",
      "Epoch: 275, loss: 0.1847\n",
      "Epoch: 276, loss: 0.2259\n",
      "Epoch: 277, loss: 0.2269\n",
      "Epoch: 278, loss: 0.2431\n",
      "Epoch: 279, loss: 0.1822\n",
      "Epoch: 280, loss: 0.2263\n",
      "Epoch: 281, loss: 0.2153\n",
      "Epoch: 282, loss: 0.2079\n",
      "Epoch: 283, loss: 0.1933\n",
      "Epoch: 284, loss: 0.1934\n",
      "Epoch: 285, loss: 0.2223\n",
      "Epoch: 286, loss: 0.2146\n",
      "Epoch: 287, loss: 0.1838\n",
      "Epoch: 288, loss: 0.1691\n",
      "Epoch: 289, loss: 0.2082\n",
      "Epoch: 290, loss: 0.2101\n",
      "Epoch: 291, loss: 0.2234\n",
      "Epoch: 292, loss: 0.2015\n",
      "Epoch: 293, loss: 0.1910\n",
      "Epoch: 294, loss: 0.1661\n",
      "Epoch: 295, loss: 0.2313\n",
      "Epoch: 296, loss: 0.1446\n",
      "Epoch: 297, loss: 0.1520\n",
      "Epoch: 298, loss: 0.1852\n",
      "Epoch: 299, loss: 0.2147\n",
      "Epoch: 300, loss: 0.2070\n",
      "Epoch: 301, loss: 0.1818\n",
      "Epoch: 302, loss: 0.1790\n",
      "Epoch: 303, loss: 0.1997\n",
      "Epoch: 304, loss: 0.1875\n",
      "Epoch: 305, loss: 0.1570\n",
      "Epoch: 306, loss: 0.1782\n",
      "Epoch: 307, loss: 0.1847\n",
      "Epoch: 308, loss: 0.1957\n",
      "Epoch: 309, loss: 0.1745\n",
      "Epoch: 310, loss: 0.1890\n",
      "Epoch: 311, loss: 0.2314\n",
      "Epoch: 312, loss: 0.1742\n",
      "Epoch: 313, loss: 0.1703\n",
      "Epoch: 314, loss: 0.2132\n",
      "Epoch: 315, loss: 0.2044\n",
      "Epoch: 316, loss: 0.1864\n",
      "Epoch: 317, loss: 0.1606\n",
      "Epoch: 318, loss: 0.2058\n",
      "Epoch: 319, loss: 0.1848\n",
      "Epoch: 320, loss: 0.2222\n",
      "Epoch: 321, loss: 0.1616\n",
      "Epoch: 322, loss: 0.2054\n",
      "Epoch: 323, loss: 0.1852\n",
      "Epoch: 324, loss: 0.2500\n",
      "Epoch: 325, loss: 0.1783\n",
      "Epoch: 326, loss: 0.2067\n",
      "Epoch: 327, loss: 0.1622\n",
      "Epoch: 328, loss: 0.2236\n",
      "Epoch: 329, loss: 0.2164\n",
      "Epoch: 330, loss: 0.1836\n",
      "Epoch: 331, loss: 0.1804\n",
      "Epoch: 332, loss: 0.1577\n",
      "Epoch: 333, loss: 0.1940\n",
      "Epoch: 334, loss: 0.1848\n",
      "Epoch: 335, loss: 0.1753\n",
      "Epoch: 336, loss: 0.1622\n",
      "Epoch: 337, loss: 0.1707\n",
      "Epoch: 338, loss: 0.1880\n",
      "Epoch: 339, loss: 0.1815\n",
      "Epoch: 340, loss: 0.1767\n",
      "Epoch: 341, loss: 0.2155\n",
      "Epoch: 342, loss: 0.1793\n",
      "Epoch: 343, loss: 0.1529\n",
      "Epoch: 344, loss: 0.1764\n",
      "Epoch: 345, loss: 0.1835\n",
      "Epoch: 346, loss: 0.1913\n",
      "Epoch: 347, loss: 0.1820\n",
      "Epoch: 348, loss: 0.2213\n",
      "Epoch: 349, loss: 0.1316\n",
      "Epoch: 350, loss: 0.2017\n",
      "Epoch: 351, loss: 0.2075\n",
      "Epoch: 352, loss: 0.2028\n",
      "Epoch: 353, loss: 0.1506\n",
      "Epoch: 354, loss: 0.2127\n",
      "Epoch: 355, loss: 0.1429\n",
      "Epoch: 356, loss: 0.2007\n",
      "Epoch: 357, loss: 0.1764\n",
      "Epoch: 358, loss: 0.2062\n",
      "Epoch: 359, loss: 0.1586\n",
      "Epoch: 360, loss: 0.1746\n",
      "Epoch: 361, loss: 0.1584\n",
      "Epoch: 362, loss: 0.2130\n",
      "Epoch: 363, loss: 0.1776\n",
      "Epoch: 364, loss: 0.2328\n",
      "Epoch: 365, loss: 0.1503\n",
      "Epoch: 366, loss: 0.1786\n",
      "Epoch: 367, loss: 0.1722\n",
      "Epoch: 368, loss: 0.1764\n",
      "Epoch: 369, loss: 0.1717\n",
      "Epoch: 370, loss: 0.1559\n",
      "Epoch: 371, loss: 0.1792\n",
      "Epoch: 372, loss: 0.1647\n",
      "Epoch: 373, loss: 0.1524\n",
      "Epoch: 374, loss: 0.1767\n",
      "Epoch: 375, loss: 0.1855\n",
      "Epoch: 376, loss: 0.1804\n",
      "Epoch: 377, loss: 0.1531\n",
      "Epoch: 378, loss: 0.1404\n",
      "Epoch: 379, loss: 0.2072\n",
      "Epoch: 380, loss: 0.1616\n",
      "Epoch: 381, loss: 0.1691\n",
      "Epoch: 382, loss: 0.2098\n",
      "Epoch: 383, loss: 0.1342\n",
      "Epoch: 384, loss: 0.1931\n",
      "Epoch: 385, loss: 0.1946\n",
      "Epoch: 386, loss: 0.1891\n",
      "Epoch: 387, loss: 0.1824\n",
      "Epoch: 388, loss: 0.2528\n",
      "Epoch: 389, loss: 0.1854\n",
      "Epoch: 390, loss: 0.1527\n",
      "Epoch: 391, loss: 0.1588\n",
      "Epoch: 392, loss: 0.1583\n",
      "Epoch: 393, loss: 0.1693\n",
      "Epoch: 394, loss: 0.2086\n",
      "Epoch: 395, loss: 0.1594\n",
      "Epoch: 396, loss: 0.1535\n",
      "Epoch: 397, loss: 0.1874\n",
      "Epoch: 398, loss: 0.1583\n",
      "Epoch: 399, loss: 0.1982\n",
      "Epoch: 400, loss: 0.1461\n",
      "Epoch: 401, loss: 0.1541\n",
      "Epoch: 402, loss: 0.1849\n",
      "Epoch: 403, loss: 0.1843\n",
      "Epoch: 404, loss: 0.2124\n",
      "Epoch: 405, loss: 0.2120\n",
      "Epoch: 406, loss: 0.1632\n",
      "Epoch: 407, loss: 0.1996\n",
      "Epoch: 408, loss: 0.1572\n",
      "Epoch: 409, loss: 0.1661\n",
      "Epoch: 410, loss: 0.1605\n",
      "Epoch: 411, loss: 0.1647\n",
      "Epoch: 412, loss: 0.1362\n",
      "Epoch: 413, loss: 0.1495\n",
      "Epoch: 414, loss: 0.1733\n",
      "Epoch: 415, loss: 0.2358\n",
      "Epoch: 416, loss: 0.1423\n",
      "Epoch: 417, loss: 0.1770\n",
      "Epoch: 418, loss: 0.2125\n",
      "Epoch: 419, loss: 0.1772\n",
      "Epoch: 420, loss: 0.1825\n",
      "Epoch: 421, loss: 0.1830\n",
      "Epoch: 422, loss: 0.1466\n",
      "Epoch: 423, loss: 0.1534\n",
      "Epoch: 424, loss: 0.1804\n",
      "Epoch: 425, loss: 0.2082\n",
      "Epoch: 426, loss: 0.1560\n",
      "Epoch: 427, loss: 0.1645\n",
      "Epoch: 428, loss: 0.2043\n",
      "Epoch: 429, loss: 0.1876\n",
      "Epoch: 430, loss: 0.2108\n",
      "Epoch: 431, loss: 0.1496\n",
      "Epoch: 432, loss: 0.1431\n",
      "Epoch: 433, loss: 0.1282\n",
      "Epoch: 434, loss: 0.2004\n",
      "Epoch: 435, loss: 0.1474\n",
      "Epoch: 436, loss: 0.1417\n",
      "Epoch: 437, loss: 0.1515\n",
      "Epoch: 438, loss: 0.1605\n",
      "Epoch: 439, loss: 0.1485\n",
      "Epoch: 440, loss: 0.1466\n",
      "Epoch: 441, loss: 0.1733\n",
      "Epoch: 442, loss: 0.1592\n",
      "Epoch: 443, loss: 0.1863\n",
      "Epoch: 444, loss: 0.1870\n",
      "Epoch: 445, loss: 0.1500\n",
      "Epoch: 446, loss: 0.1753\n",
      "Epoch: 447, loss: 0.1578\n",
      "Epoch: 448, loss: 0.1461\n",
      "Epoch: 449, loss: 0.1636\n",
      "Epoch: 450, loss: 0.2038\n",
      "Epoch: 451, loss: 0.2000\n",
      "Epoch: 452, loss: 0.1998\n",
      "Epoch: 453, loss: 0.1408\n",
      "Epoch: 454, loss: 0.1630\n",
      "Epoch: 455, loss: 0.1829\n",
      "Epoch: 456, loss: 0.2000\n",
      "Epoch: 457, loss: 0.1919\n",
      "Epoch: 458, loss: 0.1969\n",
      "Epoch: 459, loss: 0.1493\n",
      "Epoch: 460, loss: 0.1853\n",
      "Epoch: 461, loss: 0.1523\n",
      "Epoch: 462, loss: 0.1379\n",
      "Epoch: 463, loss: 0.1939\n",
      "Epoch: 464, loss: 0.1706\n",
      "Epoch: 465, loss: 0.1748\n",
      "Epoch: 466, loss: 0.1807\n",
      "Epoch: 467, loss: 0.1631\n",
      "Epoch: 468, loss: 0.1786\n",
      "Epoch: 469, loss: 0.1627\n",
      "Epoch: 470, loss: 0.1329\n",
      "Epoch: 471, loss: 0.1870\n",
      "Epoch: 472, loss: 0.1564\n",
      "Epoch: 473, loss: 0.1755\n",
      "Epoch: 474, loss: 0.1470\n",
      "Epoch: 475, loss: 0.1628\n",
      "Epoch: 476, loss: 0.1359\n",
      "Epoch: 477, loss: 0.1858\n",
      "Epoch: 478, loss: 0.1417\n",
      "Epoch: 479, loss: 0.1777\n",
      "Epoch: 480, loss: 0.1598\n",
      "Epoch: 481, loss: 0.1825\n",
      "Epoch: 482, loss: 0.1698\n",
      "Epoch: 483, loss: 0.1842\n",
      "Epoch: 484, loss: 0.1724\n",
      "Epoch: 485, loss: 0.1415\n",
      "Epoch: 486, loss: 0.1737\n",
      "Epoch: 487, loss: 0.1531\n",
      "Epoch: 488, loss: 0.1691\n",
      "Epoch: 489, loss: 0.1769\n",
      "Epoch: 490, loss: 0.1653\n",
      "Epoch: 491, loss: 0.1589\n",
      "Epoch: 492, loss: 0.1339\n",
      "Epoch: 493, loss: 0.1860\n",
      "Epoch: 494, loss: 0.1569\n",
      "Epoch: 495, loss: 0.1450\n",
      "Epoch: 496, loss: 0.1557\n",
      "Epoch: 497, loss: 0.2015\n",
      "Epoch: 498, loss: 0.1464\n",
      "Epoch: 499, loss: 0.1283\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01 ,weight_decay= 5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])        # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_correct = pred[data.test_mask]==data.y[data.test_mask]\n",
    "    test_acc = int(test_correct.sum())/int(data.test_mask.sum())\n",
    "    return test_acc\n",
    "\n",
    "for epoch in range(1,500):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.6010\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy : {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Graph Neural Network (GNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 16)\n",
      "  (out): Linear(in_features=16, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channel):\n",
    "        super(GCN,self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channel)\n",
    "        self.conv2 = GCNConv(hidden_channel, hidden_channel)\n",
    "        self.out = Linear(hidden_channel, dataset.num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x= self.conv1(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        \n",
    "        x = F.softmax(self.out(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = GCN(hidden_channel=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  00,   loss:  1.2787\n",
      "Epoch:  100,   loss:  1.2656\n",
      "Epoch:  200,   loss:  1.2520\n",
      "Epoch:  300,   loss:  1.2707\n",
      "Epoch:  400,   loss:  1.2341\n",
      "Epoch:  500,   loss:  1.2201\n",
      "Epoch:  600,   loss:  1.2380\n",
      "Epoch:  700,   loss:  1.2212\n",
      "Epoch:  800,   loss:  1.2263\n",
      "Epoch:  900,   loss:  1.2491\n",
      "Epoch:  1000,   loss:  1.2566\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x ,data.edge_index)\n",
    "    loss = criterion(out[data.train_mask],data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_correct = pred[data.test_mask]==data.y[data.test_mask]\n",
    "    test_acc = int(test_correct.sum())/int(data.test_mask.sum())\n",
    "    return test_acc\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1001):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    if epoch%100 ==0:\n",
    "        print(f'Epoch: {epoch: 03d},   loss: {loss: .4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7640\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
